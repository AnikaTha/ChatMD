Resetting modules to system default. Reseting $MODULEPATH back to system default. All extra directories will be removed from $MODULEPATH.
[NbConvertApp] Converting notebook ./notebooks/tiny_llama.ipynb to python
[NbConvertApp] Writing 3928 bytes to notebooks/tiny_llama.py
Running LORA
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
using cuda
trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.10229075496156657
Traceback (most recent call last):
  File "/home/matthewz03/git/ChatMD/./notebooks/tiny_llama.py", line 151, in <module>
    training_args = TrainingArguments(
                    ^^^^^^^^^^^^^^^^^^
  File "<string>", line 115, in __init__
  File "/home/matthewz03/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/training_args.py", line 1376, in __post_init__
    raise ValueError(
ValueError: Your setup doesn't support bf16/gpu. You need torch>=1.10, using Ampere GPU with cuda>=11.0
